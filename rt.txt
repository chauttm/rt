must use conda environment rt for Thang's rt project
https://docs.anaconda.com/working-with-conda/environments/

eg conda create -n myenvironment python numpy pandas

\> conda activate rt


when done
\> conda deactivate

conda install tensorflow 
conda install pandas

=============training=================
python rt.py train -data deepdia -logs logs-train-deepdia -epochs 2 -n_layers 4 -n_heads 8 -dropout 0.1 -batch_size 64 -d_model 64 -d_ff 256


=============test=================
python rt.py predict deepdia-b64-dm64-df256-nl4-nh8-dr0.1-ep2/ -data deepdia -output output.txt

 Layer (type)                Output Shape              Param #
=================================================================
 input (InputLayer)          [(None, 51)]              0

 transformer (encoder_block)  (None, 51, 64)           204736

 CLS_token (SlicingOpLambda)  (None, 64)               0

 predict_1 (Dense)           (None, 512)               33280

 predict_dropout_1 (Dropout)  (None, 512)              0

 predict_2 (Dense)           (None, 512)               262656

 predict_dropout_2 (Dropout)  (None, 512)              0

 output (Dense)              (None, 1)                 513

=================================================================
Total params: 501,185
Trainable params: 501,185
Non-trainable params: 0
_________________________________________________________________
2024-10-21 11:27:35.038151: E tensorflow/core/framework/node_def_util.cc:675] NodeDef mentions attribute epsilon which is not in the op definition: Op<name=_MklFusedBatchMatMulV2; signature=x:T, y:T, args:num_args*T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=num_args:int,min=0; attr=fused_ops:list(string),default=[]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node rt_transformer/transformer/encoder_layer/multi_head_attention/add}}
697/697 [==============================] - 10s 13ms/step

Model epoch = 1 ; MAE = 7.235531957621369


=============== porting to pytorch =====================================================

python train.py --device=cpu --compile=False --eval_iters=10 --log_interval=10 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=200 --lr_decay_iters=200 --dropout=0.0 --dff=256 --epochs=2 --eval_iterval=20


before the predict-head: mean instead of [0]
https://www.packtpub.com/en-us/learning/how-to-tutorials/text-classification-with-transformers?srsltid=AfmBOorRMlMqQkD1Ipt8FQSzk1MQxTra-yR-6MHDTG_O8bV_81Dewy0A
output.mean(dim=1): The purpose of using mean is to aggregate the information from the sequence into a compact representation before feeding it into self.classifier_head. It helps in reducing the spatial dimensionality and extracting the most important features from the sequence. Given the input shape (batch_size, block_size, embeds_size), the output shape is (batch_size, embeds_size). So, one fixed-size vector for each batch.

python .\predict.py --device=cpu


###======================deepdia-b64-dm64-df256-nl4-nh8-dr0.1-ep2/
python train.py --device=cpu --compile=False --eval_iters=10 --log_interval=100000 --batch_size=64 --n_layer=4 --n_head=8 --n_embd=64 --max_iters=200 --lr_decay_iters=200 --dropout=0.1 --dff=256 --epochs=2 --eval_interval=20

Initial val loss 0.1367
epoch: 1, train loss: 0.0495, val loss 0.0335
saving checkpoint to out
epoch: 2, train loss: 0.0305, val loss 0.0291
saving checkpoint to out

python .\predict.py --device=cpu
number of parameters: 0.49M
                          value
parameter
data                    deepdia
batch_size                   64
d_model                      64
d_ff                        256
n_layers                      4
n_heads                       8
dropout                     0.1
epochs                        2
vocab_size                   22
max_length                   51
min_val     -0.9466131847100333
max_val      1.9408807302384314
torch.Size([22286, 51]) torch.Size([22286, 1])
Predict loss: 0.2901

Model epoch = 2 ; MAE = 13.015941205765785

                          value
parameter
data                    deepdia
batch_size                   64
d_model                      64
d_ff                        256
n_layers                      4
n_heads                       8
dropout                     0.1
epochs                        2
vocab_size                   22
max_length                   51
min_val     -0.9466131847100333
max_val      1.9408807302384314
torch.Size([22286, 51]) torch.Size([22286, 1])
Predict loss: 0.2576

Model epoch = 2 ; MAE = 6.467903889952847

Notes
- learning rate (rt: 0.9 0.98 1e-9) a bit different from Karpathy's
- ctx context manager
- resume training from checkpoint
python train.py --device=cpu --compile=False --batch_size=64 --n_layer=4 --n_head=8 --n_embd=64 --lr_decay_iters=200 --dropout=0.1 --dff=256 --epochs=10
python train.py --init_from=resume --device=cpu --compile=False --batch_size=64 --n_layer=4 --n_head=8 --n_embd=64 --lr_decay_iters=200 --dropout=0.1 --dff=256

-----gradient_accumulation_steps
Initial val loss 0.1367
epoch: 1, train loss: 0.1324, val loss 0.1294
epoch: 2, train loss: 0.1156, val loss 0.0997
epoch: 3, train loss: 0.0928, val loss 0.0817
epoch: 4, train loss: 0.0712, val loss 0.0919
epoch: 5, train loss: 0.0837, val loss 0.0725
epoch: 6, train loss: 0.0562, val loss 0.0493
epoch: 7, train loss: 0.0490, val loss 0.0576
epoch: 8, train loss: 0.0473, val loss 0.0419
epoch: 9, train loss: 0.0442, val loss 0.0387
epoch: 10, train loss: 0.0392, val loss 0.0507

---NO--gradient_accumulation_steps
Initial val loss 0.1367
epoch: 1, train loss: 0.0495, val loss 0.0335
epoch: 2, train loss: 0.0305, val loss 0.0291
epoch: 3, train loss: 0.0299, val loss 0.0252
epoch: 4, train loss: 0.0290, val loss 0.0365
epoch: 5, train loss: 0.0283, val loss 0.0287
epoch: 6, train loss: 0.0279, val loss 0.0286
epoch: 7, train loss: 0.0277, val loss 0.0251
epoch: 8, train loss: 0.0271, val loss 0.0248
epoch: 9, train loss: 0.0265, val loss 0.0288
epoch: 10, train loss: 0.0264, val loss 0.0271

Predict loss: 0.2588

Model epoch = 10 ; MAE = 5.861498045984305




============--running with full config to see how it compares with paper results
machine: Thang

python train.py --batch_size=256 --n_layer=10 --n_head=8 --n_embd=256 --dropout=0.1 --dff=1024 --epochs=1000
epoch: 979, train loss: 0.0063, val loss 0.0135
epoch: 980, train loss: 0.0064, val loss 0.0128
epoch: 981, train loss: 0.0064, val loss 0.0148
epoch: 982, train loss: 0.0064, val loss 0.0126
epoch: 983, train loss: 0.0064, val loss 0.0127
epoch: 984, train loss: 0.0066, val loss 0.0119
epoch: 985, train loss: 0.0065, val loss 0.0130
epoch: 986, train loss: 0.0065, val loss 0.0124
epoch: 987, train loss: 0.0065, val loss 0.0147
epoch: 988, train loss: 0.0065, val loss 0.0153
epoch: 989, train loss: 0.0064, val loss 0.0133
epoch: 990, train loss: 0.0064, val loss 0.0138
epoch: 991, train loss: 0.0067, val loss 0.0136
epoch: 992, train loss: 0.0064, val loss 0.0124
epoch: 993, train loss: 0.0065, val loss 0.0125
epoch: 994, train loss: 0.0064, val loss 0.0151
epoch: 995, train loss: 0.0063, val loss 0.0128
epoch: 996, train loss: 0.0065, val loss 0.0133
epoch: 997, train loss: 0.0065, val loss 0.0123
epoch: 998, train loss: 0.0065, val loss 0.0119
epoch: 999, train loss: 0.0064, val loss 0.0155
epoch: 1000, train loss: 0.0063, val loss 0.0130

predict:
torch.Size([22286, 51]) torch.Size([22286, 1])
Predict loss: 0.2581

Model epoch = 1000 ; MAE = tensor(2.4395, device='cuda:0', dtype=torch.float64)


===================running autort LOCAL small config
python train.py --device=cpu --compile=False --batch_size=64 --n_layer=4 --n_head=8 --n_embd=64 --lr_decay_iters=200 --dropout=0.1 --dff=256 --epochs=10 --dataset=autort
...
epoch: 10, train loss: 0.0261, val loss 0.0289

python .\predict.py --device=cpu
Predict loss: 0.0292

Model epoch = 10 ; MAE = tensor(2.2865, dtype=torch.float64)



============todo:==================
- gradient_accumulation_steps - probably not
- ddp
- bias
- model directory when predict
- data directory when train
- lr_decay_iters
- memory not enough in Thang's server